### 映射（MAP）-归约(REDUCE)：大型集群应用下简化数据解析

#### 摘要

​		**MapReduce** 是一种为解析生成大规模数据集而实现的编程模型。用户指定一个**map**函数，用于解析 **k/v** 键值对数据集并生成新（中间）的 **k/v** 键值对数据集；同时指定 **reduce** 函数用于合并所有包含相同中间键的中间值（新的键值对）。如本文所述，许多现实世界中的任务均可用该模型表述。

​		这种函数式风格的程序会自动并行的运行在大型商业集群环境下。运行系统负责切分输入数据的详情，调度程序在集群机器上执行，处理机器故障及内部机器间的通信。这使没有并行和分布式系统经验的工程师可以轻松利用大型分布式系统的资源。

​		我们实现的 **MapReduce** 运行在大型商业集群环境上，并且可高度扩展；一个典型的 **MapReduce** 计算可以在上千台机器上处理上 **TB** 的数据。工程师发现该系统非常易用：已经实施了数百个 **MapReduce** 程序，并且谷歌的集群上每天都执行着上千的 **MapReduce** 任务。

#### 1. 介绍

​		在过去五年里，作者们在谷歌已经实现了数百种特殊用途的计算，用于处理大量原始数据，比如爬虫文档、**web** 请求日志等。计算各种各样的衍生数据，比如倒排索引、各种 **web** 文档的图行结构声明、每个主机爬取的页面摘要、给定时间的高频查询集合等。大多数此类计算在概念上都是易懂的，然而输入数据通常较大，计算任务不得不被分布到成百上千的机器上保证在适当的时间内去完成。如何保证并行计算、分布数据、故障处理？解决这些问题会产生大量复杂代码并掩盖原本简单的计算操作。

​		为了应对这种复杂性。我们设计了一种能够表述我们试图执行的简单计算的抽象，但是我们将并行计算、容错、数据分布以及负载均衡的细节处理隐藏在库中。我们的抽象灵感来自 **map** 和 **reduce** 在 **Lisp** 和其它函数式编程语言的早期实现。我们意识到大部分计算都涉及对输入数据中的逻辑记录应用 **map** 操作，以便计算得到一组中间 **k/v** 键值对，然后对所有包含相同键的值进行 **reduce** 操作，以便适当的组合这些导出数据。我们对用户自定义的 **map** 和 **reduce** 操作使用函数模型，这使得我们能够轻松的进行大型并行计算，同时让重新执行作为容错的主要机制。

​		这项工作的主要共享是一个简单且强大的接口，这个接口保证了大规模计算的自动并行和分布式操作，结合该接口的实现可以在大规模商业PC机器上保证高性能。

​		第二节描述了编程模型的基础，并给出了几个示例。第三节描述了一个基于集群计算环境下 **MapReduce** 接口的实现。第四节描述了我们针对编程模型已发现的几点有效改进。第五节有针对几项任务的性能衡量。第六节探讨了 **MapReduce** 在 **Google** 中的用法，包括我们使用它作为重写生产索引系统基础的经验。第七节讨论未来相关的工作。

#### 2. 编程模型

​		计算任务接收一个 **k/v** 键值对集合作为输入，并生成一个新的 **k/v** 键值对集合作为输出，**MapReduce** 库通过 **Map** 和 **Reduce** 函数表示计算。

​		用户编写的 **Map** 函数接收一个输入对并生成一个中间的 **k/v** 键值对。**MapReduce ** 库会将所有拥有相同中间键 ***I*** 的中间值分组并传递给 **Reduce ** 函数。

​		**Reduce ** 函数同样由用户编写，该函数接收一个中间键 ***I*** 和与该中间键关联的值集合，它将这些值合并在一起以形成可能较小的一组值。

​		通常一次 **Reduce ** 调用只会生成零个或一个输出值。中间值通过迭代器提供给用户的 **Reduce ** 函数。这使我们能够处理太大而无法容纳在内存中的值列表。

##### 2.1 示例

​		考虑对大量文档中每个单词的出现次数进行计数的问题。用户将编写类似如下的伪代码：

```
map(String key, String value):
    // key: document name
    // value: document contents
    for each word w in value:
        EmitIntermediate(w, "1");

reduce(String key, Iterator values):
    // key: a word
    // values: a list of counts
    int result = 0;
    for each v in values:
        result += ParseInt(v);
        Emit(AsString(result));
```

​		**map** 函数会发出每个单词以及相关的出现次数（在此简单示例中为“ 1”）。**reduce** 函数将针对特定单词发出的所有计数求和。

​		另外，用户编写代码以满足 **MapReduce** 对象规范，其中包含输入和输出文件的名称以及可选的调整参数。然后用户调用 **MapReduce** 函数，并向其传递规范对象。用户的代码被链接到 **MapReduce** 库（由 **C++**实现）。附录A包含本示例的完整代码。

##### 2.2 类型

​		即使前面的伪代码是根据字符串输入和输出编写的，从概念上来说用户提供的 **map** 和 **reduce** 函数具有关联类型：

```
map (k1,v1) → list(k2,v2)
reduce (k2,list(v2)) → list(v2)
```

​		即使输入键值和输出键值是由不同领域描绘的，但是中间键值和输出键值是来自同一领域的。

​		我们的 **C++** 实现将字符串往返传递给用户的自定义函数，并将其留给用户代码，使用户代码能够在字符串和适当的类型间转换。

##### 2.3 更多示例

​		这里有一些有趣且简单的示例，这些示例可以很容易描述 **MapReduce** 计算。

* 分布式 **grep**

  如果与提供的模式匹配，**map** 函数发出一行。**reduce** 函数是一个标识函数，它仅将提供的中间数据复制到输出中。

* 统计 **URL** 访问频率

  **map** 函数处理 **web** 页面请求日志并输出 **⟨URL, 1⟩** 键值对。**reduce** 函数将相同 **URL** 请求的值加在一起，并发出 **⟨URL, total count⟩** 键值对。

* 反转网页链接图

  **map** 函数为每一个链接输出 **⟨target,source⟩** 键值对到名为 **source** 页面中的目标 **URL**。**reduce** 函数连接与给定目标 **URL** 关联的所有源 **URL** 列表，并发出 **⟨target, list(source)⟩** 键值对。

* 主机 **Term-Vector**

  **Term-Vector** 是将一篇文档或一组文档中出现的重要词汇归总为一组 **⟨word,frequency⟩** 的键值对。**map** 函数为每一篇输入文档发出 **⟨hostname, term vector⟩** 的键值对（其中 **hostname** 是从文档的 **URL** 中导出的）。**reduce** 函数将每篇文档的 **Term-Vector** 传递给给定主机。它将 **Term-Vector** 相加，并丢弃不常用的术语，最后发出 **⟨hostname, term vector⟩** 键值对。

* 倒排索引

  **map** 函数解析每篇文章并发出一系列 **⟨word, document ID⟩** 键值对。**reduce** 函数接受给定单词的所有键值对，并对相应文档 **ID** 进行排序，然后发出 **⟨word,list(document ID)⟩** 键值对。所有输出的键值对集合形成一个简单的倒排索引。此计算易于扩展以跟踪单词位置。

* 分布式排序

  **map** 函数取出每条记录的 **key**，并发出 **⟨key, record⟩** 键值对。**reduce** 函数原样发出所有键值对。这种计算取决于第4.1节中描述的分区功能和第4.2节中描述的排序属性。

#### 3. 实现

​		**MapReduce** 接口的许多不同实现都是可能的。正确的选择取决于环境。例如，一种实现可能适用于小型共享内存型计算机，另一种实现适用于大型**NUMA(非统一内存访问 Non Uniform Memory Access Architecture)**多处理器，而另一种实现适用于网络计算机的更大集合。

​		本部分介绍了一种针对 **Google** 广泛使用的计算环境的实现：通过交换式以太网连接在一起的大型商用**PC**集群。在我们的环境中：

​		(1) 机器通常是运行的**X86**双处理器 **Linux** 系统，每台机器具有 **2-4 GB** 的内存。

​		(2) 使用商品网络硬件 --- 在机器级别通常为**100M/S**或**1G/S**，但平均对分带宽要小得多。 

​        (3) 集群由成百上千的机器组成，因此机器故障很常见。

​        (4) 廉价的 **IDE(Integrated Drive Electronics)** 磁盘直接连接到各个计算机，从而提供存储。内部开发的分			 布式文件系统[8]用于管理存储在这些磁盘上的数据。文件系统使用副本在不可靠的硬件上提供可用性和可		     靠性。

 	   (5) 用户将作业提交到调度系统。每个作业由一组任务组成，并由调度程序映射到集群中的一组可用的计算机  			 上。

##### 3.1 执行概述

​	![执行概况](./Exection overview.jpg "图1")

​		**Map** 通过调用分布在多台机器自动德将输入数据拆分到一组包含 **M** 个的切片中。输入数据的拆分可由不同的机器并行处理。***Reduce*** 分布式地调用切分函数**(如：hash(key) % R )**将中间键划分为 **R** 个片段。**分区数(R)**和分区函数由用户指定。

​		图1显示了我们实现中**MapReduce**操作的总体流程。当用户程序调用**MapReduce**函数时，将发生以下操作序列（图1中的编号标签与以下列表中的数字相对应）：

1. 用户程序中的**MapReduce**库首先将输入文件分成**M**个文件，每个文件通常为**16M**至**64M**（可由用户通过可选参数进行控制）。然后在计算机群集上启动该程序的多个副本。

2.  该程序的副本之一是特殊的 --- **master**。其余的是由**master**分配工作的**worker**。要分配到**M**个**Map worker**和R个**Reduce worker**。主机选择空闲的**worker**节点，并为每个**worker**分配一个**map**任务或一个**reduce**任务。

3.  **map worker** 将读取相应输入拆分的内容。它从输入数据中解析**k/v**对，并将**k/v**对传递给用户定义的**Map**函数。由**Map**函数产生的中间**k/v**对被缓存在内存中。

4. 缓冲对被将定期写入本地磁盘，并通过分区函数划分为**R**个区域。这些缓冲对在本地磁盘上的位置被传递回主服务器，主服务器负责将这些位置转发给**reduce worker**。

5.  当主服务器通知**reduce worker**这些缓存数据的位置时，**reduce worker**它将使用远程过程调用从**map worker**的本地磁盘中读取缓冲的数据。当**reduce worker**读取完所有中间数据时，它将按中间键对所有数据进行排序，以便将拥有相同键的所有值都分在一组。之所以需要排序，是因为通常有许多不同的键映射到相同的**reduce**任务。 如果中间数据量太大而无法容纳在内存中，则需要使用外部排序。

6.  **reduce worker**遍历排序后的中间数据，并将每个唯一的中间键和相应的中间值集传递给用户的**reduce**函数。**reduce**函数的输出将被附加到此**reduce**分区的最终输出文件中。

7. 完成所有**map**任务和**reduce**任务后，**master**将唤醒用户程序。此时用户程序中的**MapReduce**调用将返回到用户代码。

   成功完成后，可在**R**输出文件中使用**MapReduce**执行的输出（每个**reduce**任务一个文件，其文件名由用户指定）。通常，用户不需要将这些**R**输出文件合并为一个文件 --- 通常他们将这些文件作为输入传递给另一个**MapReduce**调用，或从另一个能够处理这些被划分为多个文件的分布式应用程序中使用它们。

##### 3.2  **master** 数据结构

​		**master** 保留几个数据结构。对于每个**map**任务和**reduce**任务，它存储任务状态（**空闲**，**进行中**或**已完成**）以及工作机器的标识（对于非空闲任务）。**master** 是传播**map**任务中间文件区域的位置到**reduce**任务的管道。因此，对于每个完成的**map**任务，**master**存储**map**任务生成的**R**个中间文件区域的位置和大小。**map**任务完成后，**master** 将接收到**map**节点的位置和大小信息的更新。信息将逐步推送给正在进行**reduce**任务的**worker**。

##### 3.3 容错能力

​		由于**MapReduce**库旨在通过使用数百或数千台计算机帮助处理大量数据，因此该库必须能够容忍机器故障。

* **worker** 失败

  **master**定期对每个**worker**执行**ping**操作。如果 **master** 在一定时间内未收到 **worker** 的任何响应，则**master**将**worker**标记为失败。由**worker**完成的所有**map**任务都将被重置为初始的空闲状态，并因此有资格被安排为其他**worker**。同样，在失败的工作程序上正在进行的任何**map**任务或**reduce**任务也将重置为空闲，并有资格被进行重新安排任务。

  完成的**map**任务在发生故障时会被重新执行，因为它们的输出存储在故障机器的本地磁盘上，因此无法访问。已完成的**reduce**任务的输出存储在全局文件系统中，因此无需重新执行。

  当首先由**worker A**执行**map**任务，然后再由**worker B**执行**map**任务（因为**A**失败）时，将向所有正在执行**reduce**任务的**worker**转发重新执行的通知。任何尚未从**worker A**读取到数据的**reduce**任务都将从**worker B**读取数据。

  **MapReduce**可以抵抗大规模的**worker**故障。例如，在一次**MapReduce**操作期间，正在运行的集群上的网络维护导致一次由80台计算机组成的组在数分钟内无法访问。**MapReduce** 的 **master**节点只是简单地重新执行了无法访问的**worker**未完成的工作，并继续取得执行进展，并最终完成**MapReduce**操作。

* **master**失败

  使**master**轻松写入上述定期检查点的**master**数据结构很容易。如果**master**死亡，则可以从最后一个检查点状态开始新的副本。但是由于只有一个**master**，因此发生故障的可能性不大；因此，如果**master**失败，我们的当前实现将中止**MapReduce**计算。客户端可以检查这种情况，并根据需要重试**MapReduce**操作。

* 存在故障的语义

  当用户提供的**map**和**reduce**函数是其输入值的确定性函数时，我们的分布式实现将产生与整个程序的无故障顺序执行相同的输出。

  我们依靠**map**和**reduce**的原子提交来实现此属性。每个正在进行的任务都会将其输出写入私有临时文件。**reduce**任务产生一个这样的临时文件，而**map**任务将产生**R**个这样的文件（每个**reduce**任务产生一个）。**map**任务完成后，**worker**会向**master**发送一条消息，并在消息中包含**R**个临时文件的名称。如果**master**重复收到有关已完成**map**任务的完成消息时，它将忽略该消息。否则，它将在**master**数据结构中记录**R**文件的名称。

  当**reduce**任务完成时，**reduce worker**自动将其临时输出文件重命名为最终输出文件。如果在多台计算机上执行相同的**reduce**任务，则将对同一最终输出文件执行多次重命名调用。我们依靠底层文件系统提供的原子重命名操作来确保最终文件系统状态仅包含一次**reduce**任务执行所产生的数据。

  我们的**map**和**reduce**操作绝大多数是确定的，在这种情况下我们的语义等同于顺序执行的事实，这使程序员很容易理解程序的行为。当**map**和/或**reduce**运算符不确定时，我们将提供较弱但仍然合理的语义。在存在不确定性运算符的情况下，特定**reduce**任务**R1**的输出等效于顺序执行该不确定性程序所产生的**R1**输出。然而，用于不同**reduce**任务**R2**的输出可以对应于由不确定性程序的不同顺序执行产生的用于**R2**的输出。

  考虑**map**任务**M**和**reduce**任务**R1**和**R2**。 令 $e(R_i)$ 为已落实的$R_i$的执行结果（正好有一个这样的执行）。之所以出现较弱的语义，是因为$e(R_1)$可能已经读取了由**M**的一次执行所产生的输出，而$e(R_2)$可能已经读取了由**M**的不同执行所产生的输出。

##### 3.4 区域性

​		在我们的计算环境中，网络带宽是相对稀缺的资源。我们通过将输入数据(**GFS[8]**管理)存储在组成集群的计算机的本地磁盘上来节省网络带宽。**GFS**将每个文件分成**64 M**的块，并在不同的计算机上存储每个块的多个副本（通常为**3**个副本）。**MapReduce**主服务器会考虑输入文件的位置信息，并尝试在包含相应输入数据副本的计算机上安排**Map**任务。如果失败，它将尝试在该任务的输入数据的副本附近计划**Map**任务（例如，在与包含数据的计算机且位于同一网络交换机的**worker**计算机上）。在集群中很大一部分的工作线程上运行大型**MapReduce**操作时，大多数输入数据都在本地读取，并且不占用网络带宽。

##### 3.5 任务粒度

​		如上所述，我们将**map**阶段细分为**M**个片段，将**reduce**阶段细分为**R**个片段。理想情况下，**M**和**R**应该比**worker**机器的数量大得多。让每个**worker**程序执行许多不同的任务可以动态的改善负载均衡，并且还可以在**worker**程序失败时加快恢复速度：它完成的许多**map**任务可以分布在所有其他**worker**程序计算机上。

​		在我们的实现中，**M**和**R**的大小有实际限制，因为**master**必须如上所述做出 $O(M+R)$调度决策，并将¥$O(M*R)$状态保持在内存中。(但是内存使用量的恒定因素很小：每个**map/reduce**任务对的状态$O(M*R)$大约包含一个字节数据。)

​		此外，**R**通常受用户约束，因为每个**reduce**任务的输出最终都存储在单独的输出文件中。实际上，我们倾向于选择**M**以便每个任务大约要输入**16 M**到**64 M**的输入数据（这样上述的局部性优化才是最有效的），并且我们将**R**设为**worker**数量的一小部分我们希望使用的机器。我们经常使用**2000**个**worker**执行$M=200000$和$R=5000$的**MapReduce**计算。

##### 3.6 任务备份

​		**MapReduce**操作超时的常见原因之一是“延迟”：一台机器需要花费非常长的时间才能完成最后几个计算中的**map**或**reduce**任务。延迟之所以出现，可能是由于多种原因。例如，磁盘损坏的计算机可能会经常遇到可纠正的错误，从而将其读取性能从**30 M / s**降低到**1 M / s**。集群调度系统可能已在计算机上调度了其他任务，由于竞争**CPU**、**内存**、**本地磁盘**或**网络带宽**，从而导致其执行**MapReduce**代码的速度较慢。我们最近遇到的一个问题是机器初始化代码中的一个错误，该错误导致了处理器缓存被禁用：受影响机器的计算速度降低了一百倍。

​	我们有一个通用的机制来缓解延迟的问题。当**MapReduce**操作接近完成时，**master**会调度其余正在进行的任务的执行备份。每当主执行或备份执行完成时，该任务就会标记为已完成。我们已经对该机制进行了调整，以使其通常将操作使用的计算资源增加不超过百分之几。我们发现这大大减少了完成大型**MapReduce**操作的时间。例如，禁用备份任务机制时，第**5.3**节中描述的排序程序需要超出**44％**的时间才能完成。

#### 4. 改进

​		尽管只需要编写**Map**和**Reduce**函数就可以满足大多数需求，但是我们发现一些扩展很有用。 这些将在本节中介绍。

##### 4.1 分割函数

##### 4.2 顺序保障

##### 4.3 合并函数

##### 4.4 输入输出类型

##### 4.5 副作用

##### 4.6 跳过糟糕记录

##### 4.7 本地执行

##### 4.8 状态信息

##### 4.9 计数器

#### 5. 性能

##### 5.1 集群配置

##### 5.2 搜索

##### 5.3 排序

##### 5.4 任务备份的效果

##### 5.5 机器故障

#### 6. 经验

##### 6.1 大规模索引

#### 7. 相关工作

#### 8. 结论

#### 致谢

#### 引用















